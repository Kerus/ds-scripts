{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=SettingWithCopyWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "FEATS_EXCLUDED = ['first_active_month', 'target', 'card_id', 'outliers',\n",
    "                  'hist_purchase_date_max', 'hist_purchase_date_min', 'hist_card_id_size',\n",
    "                  'new_purchase_date_max', 'new_purchase_date_min', 'new_card_id_size',\n",
    "                  'OOF_PRED', 'month_0','observation_date_x','observation_date_y']\n",
    "\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
    "\n",
    "# rmse\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encoder(df, nan_as_category = True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "    \n",
    "# Display/plot feature importance\n",
    "def display_importances(feature_importance_df_):\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lgbm_importances.png')\n",
    "\n",
    "# reduce memory\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df\n",
    "    \n",
    "# preprocessing train & test\n",
    "def train_test(num_rows=None):\n",
    "\n",
    "    # load csv\n",
    "    train_df = pd.read_csv('D:\\Ellunium\\elo/train.csv', index_col=['card_id'], nrows=num_rows)\n",
    "    test_df = pd.read_csv('D:\\Ellunium\\elo/test.csv', index_col=['card_id'], nrows=num_rows)\n",
    "\n",
    "    print(\"Train samples: {}, test samples: {}\".format(len(train_df), len(test_df)))\n",
    "\n",
    "    # outlier\n",
    "    train_df['outliers'] = 0\n",
    "    train_df.loc[train_df['target'] < -30, 'outliers'] = 1\n",
    "\n",
    "    # set target as nan\n",
    "    test_df['target'] = np.nan\n",
    "\n",
    "    # merge\n",
    "    df = train_df.append(test_df)\n",
    "\n",
    "    del train_df, test_df\n",
    "    gc.collect()\n",
    "\n",
    "    # to datetime\n",
    "    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
    "\n",
    "    # datetime features\n",
    "    df['quarter'] = df['first_active_month'].dt.quarter\n",
    "    df['elapsed_time'] = (datetime.datetime.today() - df['first_active_month']).dt.days\n",
    "\n",
    "    df['days_feature1'] = df['elapsed_time'] * df['feature_1']\n",
    "    df['days_feature2'] = df['elapsed_time'] * df['feature_2']\n",
    "    df['days_feature3'] = df['elapsed_time'] * df['feature_3']\n",
    "\n",
    "    df['days_feature1_ratio'] = df['feature_1'] / df['elapsed_time']\n",
    "    df['days_feature2_ratio'] = df['feature_2'] / df['elapsed_time']\n",
    "    df['days_feature3_ratio'] = df['feature_3'] / df['elapsed_time']\n",
    "\n",
    "    # one hot encoding\n",
    "    df, cols = one_hot_encoder(df, nan_as_category=False)\n",
    "\n",
    "    for f in ['feature_1','feature_2','feature_3']:\n",
    "        order_label = df.groupby([f])['outliers'].mean()\n",
    "        df[f] = df[f].map(order_label)\n",
    "\n",
    "    df['feature_sum'] = df['feature_1'] + df['feature_2'] + df['feature_3']\n",
    "    df['feature_mean'] = df['feature_sum']/3\n",
    "    df['feature_max'] = df[['feature_1', 'feature_2', 'feature_3']].max(axis=1)\n",
    "    df['feature_min'] = df[['feature_1', 'feature_2', 'feature_3']].min(axis=1)\n",
    "    df['feature_var'] = df[['feature_1', 'feature_2', 'feature_3']].std(axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def historical_transactions(num_rows=None):\n",
    "    # load csv\n",
    "    hist_df = pd.read_csv('D:\\Ellunium\\elo/historical_transactions.csv', nrows=num_rows)\n",
    "\n",
    "    # fillna\n",
    "    hist_df['category_2'].fillna(1.0,inplace=True)\n",
    "    hist_df['category_3'].fillna('A',inplace=True)\n",
    "    hist_df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
    "    hist_df['installments'].replace(-1, np.nan,inplace=True)\n",
    "    hist_df['installments'].replace(999, np.nan,inplace=True)\n",
    "\n",
    "    # trim\n",
    "    #hist_df['purchase_amount'] = hist_df['purchase_amount'].apply(lambda x: min(x, 0.8))\n",
    "    hist_df['purchase_amount'] = np.round(hist_df['purchase_amount'] / 0.00150265118 + 497.06,2)\n",
    "\n",
    "    # Y/N to 1/0\n",
    "    hist_df['authorized_flag'] = hist_df['authorized_flag'].map({'Y': 1, 'N': 0}).astype(int)\n",
    "    hist_df['category_1'] = hist_df['category_1'].map({'Y': 1, 'N': 0}).astype(int)\n",
    "    hist_df['category_3'] = hist_df['category_3'].map({'A':0, 'B':1, 'C':2})\n",
    "\n",
    "    # datetime features\n",
    "    hist_df['purchase_date'] = pd.to_datetime(hist_df['purchase_date'])\n",
    "    hist_df['month'] = hist_df['purchase_date'].dt.month\n",
    "    hist_df['day'] = hist_df['purchase_date'].dt.day\n",
    "    hist_df['hour'] = hist_df['purchase_date'].dt.hour\n",
    "    #hist_df['year'] = hist_df['purchase_date'].dt.year\n",
    "    hist_df['weekofyear'] = hist_df['purchase_date'].dt.weekofyear\n",
    "    hist_df['weekday'] = hist_df['purchase_date'].dt.weekday\n",
    "    hist_df['weekend'] = (hist_df['purchase_date'].dt.weekday >=5).astype(int)\n",
    "    \n",
    "\n",
    "    # additional features\n",
    "    hist_df['price'] = hist_df['purchase_amount'] / hist_df['installments']\n",
    "\n",
    "    #Christmas : December 25 2017\n",
    "    hist_df['Christmas_Day_2017']=(pd.to_datetime('2017-12-25')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    #Mothers Day: May 14 2017\n",
    "    hist_df['Mothers_Day_2017']=(pd.to_datetime('2017-06-04')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    #fathers day: August 13 2017\n",
    "    hist_df['fathers_day_2017']=(pd.to_datetime('2017-08-13')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    #Childrens day: October 12 2017\n",
    "    hist_df['Children_day_2017']=(pd.to_datetime('2017-10-12')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    #Valentine's Day : 12th June, 2017\n",
    "    hist_df['Valentine_Day_2017']=(pd.to_datetime('2017-06-12')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    #Black Friday : 24th November 2017\n",
    "    hist_df['Black_Friday_2017']=(pd.to_datetime('2017-11-24') - hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "\n",
    "    #2018\n",
    "    #Mothers Day: May 13 2018\n",
    "    hist_df['Mothers_Day_2018']=(pd.to_datetime('2018-05-13')-hist_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "\n",
    "    hist_df['month_diff'] = ((datetime.datetime.today() - hist_df['purchase_date']).dt.days)//30\n",
    "    hist_df['month_diff'] += hist_df['month_lag']\n",
    "\n",
    "    # additional features\n",
    "    hist_df['duration'] = hist_df['purchase_amount']*hist_df['month_diff']\n",
    "    hist_df['amount_month_ratio'] = hist_df['purchase_amount']/hist_df['month_diff']\n",
    "\n",
    "    # reduce memory usage\n",
    "    hist_df = reduce_mem_usage(hist_df)\n",
    "\n",
    "    col_unique =['subsector_id', 'merchant_id', 'merchant_category_id']\n",
    "    col_seas = ['month', 'hour', 'weekofyear', 'weekday', 'day']\n",
    "\n",
    "    aggs = {}\n",
    "    for col in col_unique:\n",
    "        aggs[col] = ['nunique']\n",
    "\n",
    "    for col in col_seas:\n",
    "        aggs[col] = ['nunique', 'mean', 'min', 'max']\n",
    "\n",
    "    aggs['purchase_amount'] = ['sum','max','min','mean','var','skew']\n",
    "    aggs['installments'] = ['sum','max','mean','var','skew']\n",
    "    aggs['purchase_date'] = ['max','min']\n",
    "    aggs['month_lag'] = ['max','min','mean','var','skew']\n",
    "    aggs['month_diff'] = ['max','min','mean','var','skew']\n",
    "    aggs['authorized_flag'] = ['mean']\n",
    "    aggs['weekend'] = ['mean'] # overwrite\n",
    "    aggs['weekday'] = ['mean'] # overwrite\n",
    "    aggs['day'] = ['nunique', 'mean', 'min'] # overwrite\n",
    "    aggs['category_1'] = ['mean']\n",
    "    aggs['category_2'] = ['mean']\n",
    "    aggs['category_3'] = ['mean']\n",
    "    aggs['card_id'] = ['size','count']\n",
    "    aggs['price'] = ['sum','mean','max','min','var']\n",
    "    aggs['Christmas_Day_2017'] = ['mean']\n",
    "    aggs['Mothers_Day_2017'] = ['mean']\n",
    "    aggs['fathers_day_2017'] = ['mean']\n",
    "    aggs['Children_day_2017'] = ['mean']\n",
    "    aggs['Valentine_Day_2017'] = ['mean']\n",
    "    aggs['Black_Friday_2017'] = ['mean']\n",
    "    aggs['Mothers_Day_2018'] = ['mean']\n",
    "    aggs['duration']=['mean','min','max','var','skew']\n",
    "    aggs['amount_month_ratio']=['mean','min','max','var','skew']\n",
    "\n",
    "    for col in ['category_2','category_3']:\n",
    "        hist_df[col+'_mean'] = hist_df.groupby([col])['purchase_amount'].transform('mean')\n",
    "        hist_df[col+'_min'] = hist_df.groupby([col])['purchase_amount'].transform('min')\n",
    "        hist_df[col+'_max'] = hist_df.groupby([col])['purchase_amount'].transform('max')\n",
    "        hist_df[col+'_sum'] = hist_df.groupby([col])['purchase_amount'].transform('sum')\n",
    "        aggs[col+'_mean'] = ['mean']\n",
    "\n",
    "    hist_df = hist_df.reset_index().groupby('card_id').agg(aggs)\n",
    "\n",
    "    # change column name\n",
    "    hist_df.columns = pd.Index([e[0] + \"_\" + e[1] for e in hist_df.columns.tolist()])\n",
    "    hist_df.columns = ['hist_'+ c for c in hist_df.columns]\n",
    "\n",
    "    hist_df['hist_purchase_date_diff'] = (hist_df['hist_purchase_date_max']-hist_df['hist_purchase_date_min']).dt.days\n",
    "    hist_df['hist_purchase_date_average'] = hist_df['hist_purchase_date_diff']/hist_df['hist_card_id_size']\n",
    "    hist_df['hist_purchase_date_uptonow'] = (datetime.datetime.today()-hist_df['hist_purchase_date_max']).dt.days\n",
    "    hist_df['hist_purchase_date_uptomin'] = (datetime.datetime.today()-hist_df['hist_purchase_date_min']).dt.days\n",
    "    \n",
    "    # reduce memory usage\n",
    "    hist_df = reduce_mem_usage(hist_df)\n",
    "    \n",
    "    \n",
    "\n",
    "    return hist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_historical_features(num_rows=None):\n",
    "    # load csv\n",
    "    hist_df = pd.read_csv('D:\\Ellunium\\elo/historical_transactions.csv', nrows=num_rows,\n",
    "                          usecols=['card_id','purchase_date','purchase_amount','merchant_category_id','month_lag'])\n",
    "    \n",
    "    #hist_df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
    "    hist_df['purchase_amount'] = hist_df['purchase_amount'].apply(lambda x: min(x, 0.8))\n",
    "    hist_df['purchase_date'] = pd.to_datetime(hist_df['purchase_date'])\n",
    "    hist_df['month'] = hist_df['purchase_date'].dt.month\n",
    "    hist_df['year'] = hist_df['purchase_date'].dt.year\n",
    "    \n",
    "    \n",
    "    # upd 16.02 Calculating rate of change purchases for each month\n",
    "    \n",
    "    hist_df['purchase_amount_new'] = np.round(hist_df['purchase_amount'] / 0.00150265118 + 497.06,2)\n",
    "    \n",
    "    ex_df = hist_df.groupby(['card_id','month_lag']).agg({'purchase_amount_new':'sum'})\\\n",
    "        .reset_index().rename(columns={'purchase_amount_new':'sum'})\n",
    "    #ex_df['sum'] = np.round(ex_df['sum']/ex_df['n_year'],2)\n",
    "    #ex_df.drop('n_year',axis=1,inplace=True)\n",
    "    \n",
    "    new_df = hist_df.groupby(['card_id']).size().reset_index(name=\"tmp\").drop('tmp',axis=1)\n",
    "    \n",
    "    month_lags = list(set(ex_df['month_lag'].values))\n",
    "    for i in month_lags:\n",
    "        new_df = new_df.merge(ex_df[ex_df['month_lag']==i].rename(columns={'sum':'ml_sum_'+str(i)}).drop('month_lag',axis=1),\n",
    "                          on='card_id',how=\"outer\")\n",
    "    \n",
    "    #hist_df = hist_df.merge(new_df,on='merchant_category_id',how=\"outer\")\n",
    "    \n",
    "    del ex_df\n",
    "    gc.collect()\n",
    "    \n",
    "    #new_df = hist_df.groupby(['card_id']).agg({'hist_sum_'+str(i):'mean' for i in range(1,13)}).reset_index()\n",
    "    \n",
    "    for i in month_lags:\n",
    "        new_df['ml_sum_'+str(i)].fillna(new_df['ml_sum_'+str(i)].mean(),inplace=True)\n",
    "    \n",
    "    # ПЕРЕНЕСТИ В РАЗДЕЛ ДОП ФЬЮЧЕРСОВ\n",
    "    #x = np.arange(1,13)\n",
    "    #y = np.roll(x,1)\n",
    "    #for i in range(12):\n",
    "    #    new_df['hist_ratio_'+str(x[i])] = np.round(new_df['hist_sum_'+str(x[i])] / new_df['hist_sum_'+str(y[i])],2)\n",
    "    \n",
    "    #hist_df.merge(new_df,on=\"card_id\",how='outer',inplace=True)\n",
    "    \n",
    "    #del new_df\n",
    "    #gc.collect()\n",
    "    \n",
    "    # upd 16.02 features with observation date\n",
    "    \n",
    "    #last_hist_transaction = hist_df.groupby('card_id').agg({'month_lag' : 'max', 'purchase_date' : 'max'}).reset_index()\n",
    "    #last_hist_transaction.columns = ['card_id', 'hist_month_lag', 'hist_purchase_date']\n",
    "    #last_hist_transaction['observation_date'] = \\\n",
    "    #last_hist_transaction.apply(lambda x: x['hist_purchase_date']  - pd.DateOffset(months=x['hist_month_lag']), axis=1)\n",
    "    #last_hist_transaction['observation_date'] = last_hist_transaction['observation_date'].dt.to_period('M')\\\n",
    "    #    .dt.to_timestamp() + pd.DateOffset(months=1)\n",
    "    #last_hist_transaction['observation_month'] = last_hist_transaction['observation_date'].dt.month\n",
    "    #last_hist_transaction.drop(['hist_month_lag','hist_purchase_date'],axis=1,inplace=True)\n",
    "    \n",
    "    #new_df = new_df.merge(last_hist_transaction,on=\"card_id\",how='outer')\n",
    "    \n",
    "    #del last_hist_transaction\n",
    "    #gc.collect()\n",
    "    new_df = reduce_mem_usage(new_df)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_merchant_transactions(num_rows=None):\n",
    "    # load csv\n",
    "    new_merchant_df = pd.read_csv('D:\\Ellunium\\elo/new_merchant_transactions.csv', nrows=num_rows)\n",
    "\n",
    "    # fillna\n",
    "    new_merchant_df['category_2'].fillna(1.0,inplace=True)\n",
    "    new_merchant_df['category_3'].fillna('A',inplace=True)\n",
    "    new_merchant_df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
    "    new_merchant_df['installments'].replace(-1, np.nan,inplace=True)\n",
    "    new_merchant_df['installments'].replace(999, np.nan,inplace=True)\n",
    "\n",
    "    # trim\n",
    "    #new_merchant_df['purchase_amount'] = new_merchant_df['purchase_amount'].apply(lambda x: min(x, 0.8))\n",
    "    new_merchant_df['purchase_amount'] = np.round(new_merchant_df['purchase_amount'] / 0.00150265118 + 497.06,2)\n",
    "\n",
    "    # Y/N to 1/0\n",
    "    new_merchant_df['authorized_flag'] = new_merchant_df['authorized_flag'].map({'Y': 1, 'N': 0}).astype(int)\n",
    "    new_merchant_df['category_1'] = new_merchant_df['category_1'].map({'Y': 1, 'N': 0}).astype(int)\n",
    "    new_merchant_df['category_3'] = new_merchant_df['category_3'].map({'A':0, 'B':1, 'C':2}).astype(int)\n",
    "\n",
    "    # datetime features\n",
    "    new_merchant_df['purchase_date'] = pd.to_datetime(new_merchant_df['purchase_date'])\n",
    "    new_merchant_df['month'] = new_merchant_df['purchase_date'].dt.month\n",
    "    new_merchant_df['day'] = new_merchant_df['purchase_date'].dt.day\n",
    "    #new_merchant_df['year'] = new_merchant_df['purchase_date'].dt.year\n",
    "    new_merchant_df['hour'] = new_merchant_df['purchase_date'].dt.hour\n",
    "    new_merchant_df['weekofyear'] = new_merchant_df['purchase_date'].dt.weekofyear\n",
    "    new_merchant_df['weekday'] = new_merchant_df['purchase_date'].dt.weekday\n",
    "    new_merchant_df['weekend'] = (new_merchant_df['purchase_date'].dt.weekday >=5).astype(int)\n",
    "    \n",
    "    \n",
    "    # additional features\n",
    "    new_merchant_df['price'] = new_merchant_df['purchase_amount'] / new_merchant_df['installments']\n",
    "\n",
    "    #Christmas : December 25 2017\n",
    "    new_merchant_df['Christmas_Day_2017']=(pd.to_datetime('2017-12-25')-new_merchant_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    #Childrens day: October 12 2017\n",
    "    new_merchant_df['Children_day_2017']=(pd.to_datetime('2017-10-12')-new_merchant_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "    #Black Friday : 24th November 2017\n",
    "    new_merchant_df['Black_Friday_2017']=(pd.to_datetime('2017-11-24') - new_merchant_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "\n",
    "    #Mothers Day: May 13 2018\n",
    "    new_merchant_df['Mothers_Day_2018']=(pd.to_datetime('2018-05-13')-new_merchant_df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
    "\n",
    "    new_merchant_df['month_diff'] = ((datetime.datetime.today() - new_merchant_df['purchase_date']).dt.days)//30\n",
    "    new_merchant_df['month_diff'] += new_merchant_df['month_lag']\n",
    "\n",
    "    # additional features\n",
    "    new_merchant_df['duration'] = new_merchant_df['purchase_amount']*new_merchant_df['month_diff']\n",
    "    new_merchant_df['amount_month_ratio'] = new_merchant_df['purchase_amount']/new_merchant_df['month_diff']\n",
    "\n",
    "    # reduce memory usage\n",
    "    new_merchant_df = reduce_mem_usage(new_merchant_df)\n",
    "\n",
    "    col_unique =['subsector_id', 'merchant_id', 'merchant_category_id']\n",
    "    col_seas = ['month', 'hour', 'weekofyear', 'weekday', 'day']\n",
    "\n",
    "    aggs = {}\n",
    "    for col in col_unique:\n",
    "        aggs[col] = ['nunique']\n",
    "\n",
    "    for col in col_seas:\n",
    "        aggs[col] = ['nunique', 'mean', 'min', 'max']\n",
    "\n",
    "    aggs['purchase_amount'] = ['sum','max','min','mean','var','skew']\n",
    "    aggs['installments'] = ['sum','max','mean','var','skew']\n",
    "    aggs['purchase_date'] = ['max','min']\n",
    "    aggs['month_lag'] = ['max','min','mean','var','skew']\n",
    "    aggs['month_diff'] = ['mean','var','skew']\n",
    "    aggs['weekend'] = ['mean']\n",
    "    aggs['month'] = ['mean', 'min', 'max']\n",
    "    aggs['weekday'] = ['mean', 'min', 'max']\n",
    "    aggs['category_1'] = ['mean']\n",
    "    aggs['category_2'] = ['mean']\n",
    "    aggs['category_3'] = ['mean']\n",
    "    aggs['card_id'] = ['size','count']\n",
    "    aggs['price'] = ['mean','max','min','var']\n",
    "    aggs['Christmas_Day_2017'] = ['mean']\n",
    "    aggs['Children_day_2017'] = ['mean']\n",
    "    aggs['Black_Friday_2017'] = ['mean']\n",
    "    aggs['Mothers_Day_2018'] = ['mean']\n",
    "    aggs['duration']=['mean','min','max','var','skew']\n",
    "    aggs['amount_month_ratio']=['mean','min','max','var','skew']\n",
    "\n",
    "    for col in ['category_2','category_3']:\n",
    "        new_merchant_df[col+'_mean'] = new_merchant_df.groupby([col])['purchase_amount'].transform('mean')\n",
    "        new_merchant_df[col+'_min'] = new_merchant_df.groupby([col])['purchase_amount'].transform('min')\n",
    "        new_merchant_df[col+'_max'] = new_merchant_df.groupby([col])['purchase_amount'].transform('max')\n",
    "        new_merchant_df[col+'_sum'] = new_merchant_df.groupby([col])['purchase_amount'].transform('sum')\n",
    "        aggs[col+'_mean'] = ['mean']\n",
    "\n",
    "    new_merchant_df = new_merchant_df.reset_index().groupby('card_id').agg(aggs)\n",
    "\n",
    "    # change column name\n",
    "    new_merchant_df.columns = pd.Index([e[0] + \"_\" + e[1] for e in new_merchant_df.columns.tolist()])\n",
    "    new_merchant_df.columns = ['new_'+ c for c in new_merchant_df.columns]\n",
    "\n",
    "    new_merchant_df['new_purchase_date_diff'] = (new_merchant_df['new_purchase_date_max']-new_merchant_df['new_purchase_date_min']).dt.days\n",
    "    new_merchant_df['new_purchase_date_average'] = new_merchant_df['new_purchase_date_diff']/new_merchant_df['new_card_id_size']\n",
    "    new_merchant_df['new_purchase_date_uptonow'] = (datetime.datetime.today()-new_merchant_df['new_purchase_date_max']).dt.days\n",
    "    new_merchant_df['new_purchase_date_uptomin'] = (datetime.datetime.today()-new_merchant_df['new_purchase_date_min']).dt.days\n",
    "\n",
    "    # reduce memory usage\n",
    "    new_merchant_df = reduce_mem_usage(new_merchant_df)\n",
    "\n",
    "    return new_merchant_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_new_merch_features(num_rows=None):\n",
    "    # load csv\n",
    "    n_m_df = pd.read_csv('D:\\Ellunium\\elo/new_merchant_transactions.csv', nrows=num_rows,\n",
    "                          usecols=['card_id','purchase_date','purchase_amount','merchant_category_id','month_lag'])\n",
    "    \n",
    "    #n_m_df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
    "    #n_m_df['purchase_amount'] = n_m_df['purchase_amount'].apply(lambda x: min(x, 0.8))\n",
    "    n_m_df['purchase_date'] = pd.to_datetime(n_m_df['purchase_date'])\n",
    "    n_m_df['month'] = n_m_df['purchase_date'].dt.month\n",
    "    n_m_df['year'] = n_m_df['purchase_date'].dt.year\n",
    "    \n",
    "    \n",
    "    # upd 16.02 Calculating rate of change purchases for each month\n",
    "    \n",
    "    n_m_df['purchase_amount_new'] = np.round(n_m_df['purchase_amount'] / 0.00150265118 + 497.06,2)\n",
    "    \n",
    "    ex_df = n_m_df.groupby(['card_id','month_lag']).agg({'purchase_amount_new':'sum'})\\\n",
    "        .reset_index().rename(columns={'purchase_amount_new':'sum'})\n",
    "    #ex_df['sum'] = np.round(ex_df['sum']/ex_df['n_year'],2)\n",
    "    #ex_df.drop('n_year',axis=1,inplace=True)\n",
    "    \n",
    "    new_df = n_m_df.groupby(['card_id']).size().reset_index(name=\"tmp\").drop('tmp',axis=1)\n",
    "    month_lags = list(set(ex_df['month_lag'].values))\n",
    "    for i in month_lags:\n",
    "        new_df = new_df.merge(ex_df[ex_df['month_lag']==i].rename(columns={'sum':'ml_sum_'+str(i)}).drop('month_lag',axis=1),\n",
    "                          on='card_id',how=\"outer\")\n",
    "    \n",
    "    #n_m_df = n_m_df.merge(new_df,on='merchant_category_id',how=\"outer\")\n",
    "    \n",
    "    del ex_df\n",
    "    gc.collect()\n",
    "    \n",
    "    #new_df = n_m_df.groupby(['card_id']).agg({'newm_sum_'+str(i):'mean' for i in range(1,13)}).reset_index()\n",
    "    \n",
    "    for i in month_lags:\n",
    "        new_df['ml_sum_'+str(i)].fillna(new_df['ml_sum_'+str(i)].mean(),inplace=True)\n",
    "    \n",
    "    #x = np.arange(1,13)\n",
    "    #y = np.roll(x,1)\n",
    "    #for i in range(12):\n",
    "    #    new_df['newm_ratio_'+str(x[i])] = np.round(new_df['newm_sum_'+str(x[i])] / new_df['newm_sum_'+str(y[i])],2)\n",
    "    \n",
    "    #hist_df.merge(new_df,on=\"card_id\",how='outer',inplace=True)\n",
    "    \n",
    "    #del new_df\n",
    "    #gc.collect()\n",
    "    \n",
    "    # upd 16.02 features with observation date\n",
    "    \n",
    "    #first_new_transaction = n_m_df.groupby('card_id').agg({'month_lag' : 'min', 'purchase_date' : 'min'}).reset_index()\n",
    "    #first_new_transaction.columns = ['card_id', 'new_month_lag', 'new_purchase_date']\n",
    "    #first_new_transaction['observation_date'] = \\\n",
    "    #    first_new_transaction.apply(lambda x: x['new_purchase_date']  - pd.DateOffset(months=x['new_month_lag']-1), axis=1)\n",
    "    #first_new_transaction['observation_date'] = first_new_transaction['observation_date'].dt.to_period('M').dt.to_timestamp()\n",
    "    \n",
    "    #first_new_transaction['observation_month'] = first_new_transaction['observation_date'].dt.month\n",
    "    #first_new_transaction.drop(['new_month_lag','new_purchase_date'],axis=1,inplace=True)\n",
    "    \n",
    "    #new_df = new_df.merge(first_new_transaction,on=\"card_id\",how='outer')\n",
    "    \n",
    "    #del first_new_transaction\n",
    "    #gc.collect()\n",
    "    new_df = reduce_mem_usage(new_df)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lag1(df):\n",
    "    df_train = df[df['ml_sum_1'].notnull()]\n",
    "    df_test = df[df['ml_sum_1'].isnull()]  \n",
    "    target = df_train['ml_sum_1']\n",
    "    del(df)\n",
    "    del(df_train['ml_sum_1'])\n",
    "    gc.collect()\n",
    "    \n",
    "    features = [c for c in df_train.columns if c not in FEATS_EXCLUDED]\n",
    "    categorical_feats = [c for c in features if 'feature_' in c]\n",
    "    \n",
    "    param = {'objective':'regression',\n",
    "         'num_leaves': 31,\n",
    "         'min_data_in_leaf': 25,\n",
    "         'max_depth': 7,\n",
    "         'learning_rate': 0.01,\n",
    "         'lambda_l1':0.13,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\":0.85,\n",
    "         'bagging_freq':8,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"metric\": 'rmse',\n",
    "         \"verbosity\": -1,\n",
    "         \"random_state\": 2333}\n",
    "\n",
    "    folds = KFold(n_splits= 5, shuffle=True, random_state=326) \n",
    "    oof_lgb = np.zeros(len(df_train))\n",
    "    predictions_lgb = np.zeros(len(df_test))\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train)):\n",
    "        print(\"fold {}\".format(fold_))\n",
    "        trn_data = lgb.Dataset(df_train.iloc[trn_idx][features], label=target.iloc[trn_idx])#, categorical_feature=categorical_feats)\n",
    "        val_data = lgb.Dataset(df_train.iloc[val_idx][features], label=target.iloc[val_idx])#, categorical_feature=categorical_feats)\n",
    "\n",
    "        num_round = 10000\n",
    "        clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval= 100, early_stopping_rounds = 200)\n",
    "        oof_lgb[val_idx] = clf.predict(df_train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"Feature\"] = features\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "        fold_importance_df[\"fold\"] = fold_ + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "\n",
    "        predictions_lgb += clf.predict(df_test[features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "    print(\"CV score: {:<8.5f}\".format(mean_squared_error(oof_lgb, target)**0.5))\n",
    "    # simple model\n",
    "    pred_ml1_df = pd.DataFrame({\"card_id\":df_test[\"card_id\"].values})\n",
    "    pred_ml1_df[\"ml_sum_1\"] = predictions_lgb\n",
    "    \n",
    "    del df_train,df_test\n",
    "    gc.collect()\n",
    "    \n",
    "    return pred_ml1_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lag2(df):\n",
    "    df_train = df[df['ml_sum_2'].notnull()]\n",
    "    df_test = df[df['ml_sum_2'].isnull()]  \n",
    "    target = df_train['ml_sum_2']\n",
    "    del(df)\n",
    "    del(df_train['ml_sum_2'])\n",
    "    gc.collect()\n",
    "    \n",
    "    features = [c for c in df_train.columns if c not in FEATS_EXCLUDED]\n",
    "    categorical_feats = [c for c in features if 'feature_' in c]\n",
    "    \n",
    "    param = {'objective':'regression',\n",
    "         'num_leaves': 31,\n",
    "         'min_data_in_leaf': 25,\n",
    "         'max_depth': 7,\n",
    "         'learning_rate': 0.01,\n",
    "         'lambda_l1':0.13,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\":0.85,\n",
    "         'bagging_freq':8,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"metric\": 'rmse',\n",
    "         \"verbosity\": -1,\n",
    "         \"random_state\": 2333}\n",
    "\n",
    "    folds = KFold(n_splits= 5, shuffle=True, random_state=326) \n",
    "    oof_lgb = np.zeros(len(df_train))\n",
    "    predictions_lgb = np.zeros(len(df_test))\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train)):\n",
    "        print(\"fold {}\".format(fold_))\n",
    "        trn_data = lgb.Dataset(df_train.iloc[trn_idx][features], label=target.iloc[trn_idx])#, categorical_feature=categorical_feats)\n",
    "        val_data = lgb.Dataset(df_train.iloc[val_idx][features], label=target.iloc[val_idx])#, categorical_feature=categorical_feats)\n",
    "\n",
    "        num_round = 10000\n",
    "        clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval= 100, early_stopping_rounds = 200)\n",
    "        oof_lgb[val_idx] = clf.predict(df_train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"Feature\"] = features\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "        fold_importance_df[\"fold\"] = fold_ + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "\n",
    "        predictions_lgb += clf.predict(df_test[features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "    print(\"CV score: {:<8.5f}\".format(mean_squared_error(oof_lgb, target)**0.5))\n",
    "    # simple model\n",
    "    pred_ml2_df = pd.DataFrame({\"card_id\":df_test[\"card_id\"].values})\n",
    "    pred_ml2_df[\"ml_sum_2\"] = predictions_lgb\n",
    "    \n",
    "    del df_train,df_test\n",
    "    gc.collect()\n",
    "    \n",
    "    return pred_ml2_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ml_lags(df):\n",
    "    pred_ml1_df = predict_lag1(df)\n",
    "    pred_ml2_df = predict_lag2(df)\n",
    "    for card_id in pred_ml1_df['card_id']:\n",
    "        df.loc[df['card_id']==card_id,'ml_sum_1'] = pred_ml1_df.loc[pred_ml1_df['card_id']==card_id,'ml_sum_1'].values\n",
    "        df.loc[df['card_id']==card_id,'ml_sum_2'] = pred_ml2_df.loc[pred_ml2_df['card_id']==card_id,'ml_sum_2'].values\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def additional_features(df):\n",
    "    df['hist_first_buy'] = (df['hist_purchase_date_min'] - df['first_active_month']).dt.days\n",
    "    df['hist_last_buy'] = (df['hist_purchase_date_max'] - df['first_active_month']).dt.days\n",
    "    df['new_first_buy'] = (df['new_purchase_date_min'] - df['first_active_month']).dt.days\n",
    "    df['new_last_buy'] = (df['new_purchase_date_max'] - df['first_active_month']).dt.days\n",
    "\n",
    "    date_features=['hist_purchase_date_max','hist_purchase_date_min',\n",
    "                   'new_purchase_date_max', 'new_purchase_date_min']\n",
    "\n",
    "    for f in date_features:\n",
    "        df[f] = df[f].astype(np.int64) * 1e-9\n",
    "    \n",
    "    \n",
    "    # ПЕРЕНЕСТИ В РАЗДЕЛ ДОП ФЬЮЧЕРСОВ\n",
    "    x = np.arange(-13,3)\n",
    "    y = np.roll(x,1)\n",
    "    for i in range(16):\n",
    "        df['ml_ratio_'+str(x[i])] = np.round(df['ml_sum_'+str(x[i])] / df['ml_sum_'+str(y[i])],2)\n",
    "    \n",
    "    # upd 16.02 Возможно стоит считать только для месяцев 2,3,4\n",
    "    #for i in range(1,13):\n",
    "    #    df['new_hist_ratio_'+str(i)] = np.round(df['newm_sum_'+str(i)] / df['hist_sum_'+str(i)],2)\n",
    "        \n",
    "    # by option\n",
    "    #df.drop(['_'.join(['newm_sum',str(j)]) for j in range(1,13)],axis=1,inplace=True)\n",
    "    #df.drop(['_'.join(['hist_sum',str(j)]) for j in range(1,13)],axis=1,inplace=True)\n",
    "    #gc.collect()\n",
    "    \n",
    "    df['card_id_total'] = df['new_card_id_size']+df['hist_card_id_size']\n",
    "    df['card_id_cnt_total'] = df['new_card_id_count']+df['hist_card_id_count']\n",
    "    df['card_id_cnt_ratio'] = df['new_card_id_count']/df['hist_card_id_count']\n",
    "    df['purchase_amount_total'] = df['new_purchase_amount_sum']+df['hist_purchase_amount_sum']\n",
    "    df['purchase_amount_mean'] = df['new_purchase_amount_mean']+df['hist_purchase_amount_mean']\n",
    "    df['purchase_amount_max'] = df['new_purchase_amount_max']+df['hist_purchase_amount_max']\n",
    "    df['purchase_amount_min'] = df['new_purchase_amount_min']+df['hist_purchase_amount_min']\n",
    "    df['purchase_amount_ratio'] = df['new_purchase_amount_sum']/df['hist_purchase_amount_sum']\n",
    "    df['month_diff_mean'] = df['new_month_diff_mean']+df['hist_month_diff_mean']\n",
    "    df['month_diff_ratio'] = df['new_month_diff_mean']/df['hist_month_diff_mean']\n",
    "    df['month_lag_mean'] = df['new_month_lag_mean']+df['hist_month_lag_mean']\n",
    "    df['month_lag_max'] = df['new_month_lag_max']+df['hist_month_lag_max']\n",
    "    df['month_lag_min'] = df['new_month_lag_min']+df['hist_month_lag_min']\n",
    "    df['category_1_mean'] = df['new_category_1_mean']+df['hist_category_1_mean']\n",
    "    df['installments_total'] = df['new_installments_sum']+df['hist_installments_sum']\n",
    "    df['installments_mean'] = df['new_installments_mean']+df['hist_installments_mean']\n",
    "    df['installments_max'] = df['new_installments_max']+df['hist_installments_max']\n",
    "    df['installments_ratio'] = df['new_installments_sum']/df['hist_installments_sum']\n",
    "    df['price_total'] = df['purchase_amount_total'] / df['installments_total']\n",
    "    df['price_mean'] = df['purchase_amount_mean'] / df['installments_mean']\n",
    "    df['price_max'] = df['purchase_amount_max'] / df['installments_max']\n",
    "    df['duration_mean'] = df['new_duration_mean']+df['hist_duration_mean']\n",
    "    df['duration_min'] = df['new_duration_min']+df['hist_duration_min']\n",
    "    df['duration_max'] = df['new_duration_max']+df['hist_duration_max']\n",
    "    df['amount_month_ratio_mean']=df['new_amount_month_ratio_mean']+df['hist_amount_month_ratio_mean']\n",
    "    df['amount_month_ratio_min']=df['new_amount_month_ratio_min']+df['hist_amount_month_ratio_min']\n",
    "    df['amount_month_ratio_max']=df['new_amount_month_ratio_max']+df['hist_amount_month_ratio_max']\n",
    "    df['new_CLV'] = df['new_card_id_count'] * df['new_purchase_amount_sum'] / df['new_month_diff_mean']\n",
    "    df['hist_CLV'] = df['hist_card_id_count'] * df['hist_purchase_amount_sum'] / df['hist_month_diff_mean']\n",
    "    df['CLV_ratio'] = df['new_CLV'] / df['hist_CLV']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 201917, test samples: 123623\n"
     ]
    }
   ],
   "source": [
    "debug = False\n",
    "num_rows = 10000 if debug else None\n",
    "df = train_test(num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 19.25 MB\n",
      "Decreased by 51.6%\n"
     ]
    }
   ],
   "source": [
    "df = pd.merge(df, rate_historical_features(num_rows), on='card_id', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 325540 entries, 0 to 325539\n",
      "Data columns (total 34 columns):\n",
      "card_id                325540 non-null object\n",
      "feature_1              325540 non-null float64\n",
      "feature_2              325540 non-null float64\n",
      "feature_3              325540 non-null float64\n",
      "first_active_month     325539 non-null datetime64[ns]\n",
      "outliers               201917 non-null float64\n",
      "target                 201917 non-null float64\n",
      "quarter                325539 non-null float64\n",
      "elapsed_time           325539 non-null float64\n",
      "days_feature1          325539 non-null float64\n",
      "days_feature2          325539 non-null float64\n",
      "days_feature3          325539 non-null float64\n",
      "days_feature1_ratio    325539 non-null float64\n",
      "days_feature2_ratio    325539 non-null float64\n",
      "days_feature3_ratio    325539 non-null float64\n",
      "feature_sum            325540 non-null float64\n",
      "feature_mean           325540 non-null float64\n",
      "feature_max            325540 non-null float64\n",
      "feature_min            325540 non-null float64\n",
      "feature_var            325540 non-null float64\n",
      "ml_sum_0               325540 non-null float32\n",
      "ml_sum_-13             325540 non-null float16\n",
      "ml_sum_-12             325540 non-null float16\n",
      "ml_sum_-11             325540 non-null float16\n",
      "ml_sum_-10             325540 non-null float32\n",
      "ml_sum_-9              325540 non-null float16\n",
      "ml_sum_-8              325540 non-null float32\n",
      "ml_sum_-7              325540 non-null float32\n",
      "ml_sum_-6              325540 non-null float32\n",
      "ml_sum_-5              325540 non-null float32\n",
      "ml_sum_-4              325540 non-null float32\n",
      "ml_sum_-3              325540 non-null float32\n",
      "ml_sum_-2              325540 non-null float16\n",
      "ml_sum_-1              325540 non-null float32\n",
      "dtypes: datetime64[ns](1), float16(5), float32(9), float64(18), object(1)\n",
      "memory usage: 66.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 6.64 MB\n",
      "Decreased by 25.0%\n"
     ]
    }
   ],
   "source": [
    "df = pd.merge(df, rate_new_merch_features(num_rows), on='card_id', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 1998.99 MB\n",
      "Decreased by 69.5%\n",
      "Memory usage after optimization is: 63.64 MB\n",
      "Decreased by 55.1%\n"
     ]
    }
   ],
   "source": [
    "df = pd.merge(df, historical_transactions(num_rows), on='card_id', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 125.43 MB\n",
      "Decreased by 67.8%\n",
      "Memory usage after optimization is: 53.38 MB\n",
      "Decreased by 52.0%\n"
     ]
    }
   ],
   "source": [
    "df = pd.merge(df, new_merchant_transactions(num_rows), on='card_id', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 544.481\tvalid_1's rmse: 509.625\n",
      "[200]\ttraining's rmse: 356.934\tvalid_1's rmse: 336.532\n",
      "[300]\ttraining's rmse: 287.366\tvalid_1's rmse: 281.972\n",
      "[400]\ttraining's rmse: 251.071\tvalid_1's rmse: 260.425\n",
      "[500]\ttraining's rmse: 226.925\tvalid_1's rmse: 251.398\n",
      "[600]\ttraining's rmse: 205.259\tvalid_1's rmse: 244.061\n",
      "[700]\ttraining's rmse: 189.352\tvalid_1's rmse: 240.38\n",
      "[800]\ttraining's rmse: 174.589\tvalid_1's rmse: 236.684\n",
      "[900]\ttraining's rmse: 163.703\tvalid_1's rmse: 234.298\n",
      "[1000]\ttraining's rmse: 153.61\tvalid_1's rmse: 231.589\n",
      "[1100]\ttraining's rmse: 144.741\tvalid_1's rmse: 229.708\n",
      "[1200]\ttraining's rmse: 136.822\tvalid_1's rmse: 228.563\n",
      "[1300]\ttraining's rmse: 130.3\tvalid_1's rmse: 227.152\n",
      "[1400]\ttraining's rmse: 123.243\tvalid_1's rmse: 226.545\n",
      "[1500]\ttraining's rmse: 117.604\tvalid_1's rmse: 226.377\n",
      "[1600]\ttraining's rmse: 111.936\tvalid_1's rmse: 226.036\n",
      "[1700]\ttraining's rmse: 107.954\tvalid_1's rmse: 225.734\n",
      "[1800]\ttraining's rmse: 103.262\tvalid_1's rmse: 225.599\n",
      "[1900]\ttraining's rmse: 98.6075\tvalid_1's rmse: 225.371\n",
      "[2000]\ttraining's rmse: 94.9229\tvalid_1's rmse: 224.895\n",
      "[2100]\ttraining's rmse: 91.5661\tvalid_1's rmse: 224.41\n",
      "[2200]\ttraining's rmse: 88.0959\tvalid_1's rmse: 224.975\n",
      "Early stopping, best iteration is:\n",
      "[2097]\ttraining's rmse: 91.6703\tvalid_1's rmse: 224.375\n",
      "fold 1\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 523.399\tvalid_1's rmse: 604.348\n",
      "[200]\ttraining's rmse: 333.349\tvalid_1's rmse: 448.229\n",
      "[300]\ttraining's rmse: 263.506\tvalid_1's rmse: 399.244\n",
      "[400]\ttraining's rmse: 230.347\tvalid_1's rmse: 379.161\n",
      "[500]\ttraining's rmse: 207.024\tvalid_1's rmse: 358.849\n",
      "[600]\ttraining's rmse: 188.809\tvalid_1's rmse: 347.898\n",
      "[700]\ttraining's rmse: 173.852\tvalid_1's rmse: 338.876\n",
      "[800]\ttraining's rmse: 161.256\tvalid_1's rmse: 334.818\n",
      "[900]\ttraining's rmse: 150.643\tvalid_1's rmse: 332.667\n",
      "[1000]\ttraining's rmse: 141.472\tvalid_1's rmse: 330.67\n",
      "[1100]\ttraining's rmse: 134.06\tvalid_1's rmse: 329.283\n",
      "[1200]\ttraining's rmse: 126.423\tvalid_1's rmse: 326.808\n",
      "[1300]\ttraining's rmse: 119.459\tvalid_1's rmse: 326.248\n",
      "[1400]\ttraining's rmse: 113.764\tvalid_1's rmse: 325.477\n",
      "[1500]\ttraining's rmse: 108.297\tvalid_1's rmse: 324.262\n",
      "[1600]\ttraining's rmse: 103.933\tvalid_1's rmse: 324.051\n",
      "[1700]\ttraining's rmse: 99.9822\tvalid_1's rmse: 322.945\n",
      "[1800]\ttraining's rmse: 95.848\tvalid_1's rmse: 322.105\n",
      "[1900]\ttraining's rmse: 91.9048\tvalid_1's rmse: 321.411\n",
      "[2000]\ttraining's rmse: 88.4882\tvalid_1's rmse: 320.611\n",
      "[2100]\ttraining's rmse: 85.6857\tvalid_1's rmse: 320.49\n",
      "[2200]\ttraining's rmse: 82.3859\tvalid_1's rmse: 320.09\n",
      "[2300]\ttraining's rmse: 80.0096\tvalid_1's rmse: 319.793\n",
      "[2400]\ttraining's rmse: 77.4048\tvalid_1's rmse: 319.587\n",
      "[2500]\ttraining's rmse: 75.0226\tvalid_1's rmse: 319.031\n",
      "[2600]\ttraining's rmse: 72.9926\tvalid_1's rmse: 318.692\n",
      "[2700]\ttraining's rmse: 70.7414\tvalid_1's rmse: 318.442\n",
      "[2800]\ttraining's rmse: 68.6794\tvalid_1's rmse: 318.521\n",
      "[2900]\ttraining's rmse: 66.9763\tvalid_1's rmse: 318.169\n",
      "[3000]\ttraining's rmse: 64.9459\tvalid_1's rmse: 317.904\n",
      "[3100]\ttraining's rmse: 63.1657\tvalid_1's rmse: 317.625\n",
      "[3200]\ttraining's rmse: 61.5163\tvalid_1's rmse: 317.736\n",
      "Early stopping, best iteration is:\n",
      "[3080]\ttraining's rmse: 63.5532\tvalid_1's rmse: 317.525\n",
      "fold 2\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 545.805\tvalid_1's rmse: 476.37\n",
      "[200]\ttraining's rmse: 353.27\tvalid_1's rmse: 304.495\n",
      "[300]\ttraining's rmse: 281.181\tvalid_1's rmse: 255.201\n",
      "[400]\ttraining's rmse: 244.707\tvalid_1's rmse: 241.008\n",
      "[500]\ttraining's rmse: 219.706\tvalid_1's rmse: 235.883\n",
      "[600]\ttraining's rmse: 198.044\tvalid_1's rmse: 230.981\n",
      "[700]\ttraining's rmse: 182.867\tvalid_1's rmse: 228.116\n",
      "[800]\ttraining's rmse: 169.271\tvalid_1's rmse: 226.814\n",
      "[900]\ttraining's rmse: 159.333\tvalid_1's rmse: 226.383\n",
      "[1000]\ttraining's rmse: 149.676\tvalid_1's rmse: 225.582\n",
      "[1100]\ttraining's rmse: 141.548\tvalid_1's rmse: 225.3\n",
      "[1200]\ttraining's rmse: 134.605\tvalid_1's rmse: 224.609\n",
      "[1300]\ttraining's rmse: 127.762\tvalid_1's rmse: 224.053\n",
      "[1400]\ttraining's rmse: 121.783\tvalid_1's rmse: 223.511\n",
      "[1500]\ttraining's rmse: 116.168\tvalid_1's rmse: 223.337\n",
      "[1600]\ttraining's rmse: 111.797\tvalid_1's rmse: 223.226\n",
      "[1700]\ttraining's rmse: 106.598\tvalid_1's rmse: 223.602\n",
      "Early stopping, best iteration is:\n",
      "[1577]\ttraining's rmse: 112.786\tvalid_1's rmse: 223.101\n",
      "fold 3\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 517.818\tvalid_1's rmse: 654.117\n",
      "[200]\ttraining's rmse: 327.308\tvalid_1's rmse: 496.727\n",
      "[300]\ttraining's rmse: 260.01\tvalid_1's rmse: 444.34\n",
      "[400]\ttraining's rmse: 229.31\tvalid_1's rmse: 423.589\n",
      "[500]\ttraining's rmse: 206.338\tvalid_1's rmse: 409.072\n",
      "[600]\ttraining's rmse: 185.873\tvalid_1's rmse: 397.641\n",
      "[700]\ttraining's rmse: 170.473\tvalid_1's rmse: 389.62\n",
      "[800]\ttraining's rmse: 158.187\tvalid_1's rmse: 385.631\n",
      "[900]\ttraining's rmse: 148.846\tvalid_1's rmse: 382.379\n",
      "[1000]\ttraining's rmse: 140.984\tvalid_1's rmse: 380.547\n",
      "[1100]\ttraining's rmse: 134.159\tvalid_1's rmse: 379.057\n",
      "[1200]\ttraining's rmse: 127.178\tvalid_1's rmse: 377.938\n",
      "[1300]\ttraining's rmse: 120.843\tvalid_1's rmse: 376.23\n",
      "[1400]\ttraining's rmse: 114.976\tvalid_1's rmse: 375.673\n",
      "[1500]\ttraining's rmse: 109.513\tvalid_1's rmse: 374.651\n",
      "[1600]\ttraining's rmse: 105.034\tvalid_1's rmse: 373.788\n",
      "[1700]\ttraining's rmse: 99.9432\tvalid_1's rmse: 372.951\n",
      "[1800]\ttraining's rmse: 96.0961\tvalid_1's rmse: 373.255\n",
      "[1900]\ttraining's rmse: 92.1764\tvalid_1's rmse: 372.508\n",
      "[2000]\ttraining's rmse: 88.3363\tvalid_1's rmse: 371.598\n",
      "[2100]\ttraining's rmse: 85.2657\tvalid_1's rmse: 371.783\n",
      "[2200]\ttraining's rmse: 82.4956\tvalid_1's rmse: 371.563\n",
      "[2300]\ttraining's rmse: 79.4098\tvalid_1's rmse: 371.287\n",
      "[2400]\ttraining's rmse: 76.9492\tvalid_1's rmse: 371.157\n",
      "[2500]\ttraining's rmse: 74.6021\tvalid_1's rmse: 371.216\n",
      "[2600]\ttraining's rmse: 72.204\tvalid_1's rmse: 370.967\n",
      "[2700]\ttraining's rmse: 69.8756\tvalid_1's rmse: 370.646\n",
      "[2800]\ttraining's rmse: 67.6211\tvalid_1's rmse: 370.424\n",
      "[2900]\ttraining's rmse: 65.4672\tvalid_1's rmse: 370.272\n",
      "[3000]\ttraining's rmse: 63.7614\tvalid_1's rmse: 370.064\n",
      "[3100]\ttraining's rmse: 61.816\tvalid_1's rmse: 369.871\n",
      "[3200]\ttraining's rmse: 60.1805\tvalid_1's rmse: 369.957\n",
      "[3300]\ttraining's rmse: 58.657\tvalid_1's rmse: 370.205\n",
      "Early stopping, best iteration is:\n",
      "[3160]\ttraining's rmse: 60.7777\tvalid_1's rmse: 369.797\n",
      "fold 4\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 538.585\tvalid_1's rmse: 518.878\n",
      "[200]\ttraining's rmse: 347.261\tvalid_1's rmse: 346.243\n",
      "[300]\ttraining's rmse: 276.351\tvalid_1's rmse: 294.633\n",
      "[400]\ttraining's rmse: 240.724\tvalid_1's rmse: 275.511\n",
      "[500]\ttraining's rmse: 217.04\tvalid_1's rmse: 265.031\n",
      "[600]\ttraining's rmse: 194.82\tvalid_1's rmse: 255.974\n",
      "[700]\ttraining's rmse: 178.114\tvalid_1's rmse: 250.053\n",
      "[800]\ttraining's rmse: 165.247\tvalid_1's rmse: 246.567\n",
      "[900]\ttraining's rmse: 155.139\tvalid_1's rmse: 245.363\n",
      "[1000]\ttraining's rmse: 146.244\tvalid_1's rmse: 243.158\n",
      "[1100]\ttraining's rmse: 137.143\tvalid_1's rmse: 242.345\n",
      "[1200]\ttraining's rmse: 129.514\tvalid_1's rmse: 241.36\n",
      "[1300]\ttraining's rmse: 122.082\tvalid_1's rmse: 241.034\n",
      "[1400]\ttraining's rmse: 115.391\tvalid_1's rmse: 240.371\n",
      "[1500]\ttraining's rmse: 110.013\tvalid_1's rmse: 239.728\n",
      "[1600]\ttraining's rmse: 104.924\tvalid_1's rmse: 240.074\n",
      "[1700]\ttraining's rmse: 99.9312\tvalid_1's rmse: 239.185\n",
      "[1800]\ttraining's rmse: 95.7188\tvalid_1's rmse: 238.825\n",
      "[1900]\ttraining's rmse: 92.1413\tvalid_1's rmse: 238.925\n",
      "[2000]\ttraining's rmse: 89.1957\tvalid_1's rmse: 238.859\n",
      "[2100]\ttraining's rmse: 85.5757\tvalid_1's rmse: 238.415\n",
      "[2200]\ttraining's rmse: 82.1407\tvalid_1's rmse: 238.199\n",
      "[2300]\ttraining's rmse: 79.0813\tvalid_1's rmse: 238.036\n",
      "[2400]\ttraining's rmse: 76.3413\tvalid_1's rmse: 237.848\n",
      "[2500]\ttraining's rmse: 73.808\tvalid_1's rmse: 237.476\n",
      "[2600]\ttraining's rmse: 71.4156\tvalid_1's rmse: 237.782\n",
      "[2700]\ttraining's rmse: 69.1156\tvalid_1's rmse: 237.759\n",
      "Early stopping, best iteration is:\n",
      "[2512]\ttraining's rmse: 73.5369\tvalid_1's rmse: 237.409\n",
      "CV score: 280.73203\n",
      "fold 0\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 645.752\tvalid_1's rmse: 507.679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttraining's rmse: 494.686\tvalid_1's rmse: 366.972\n",
      "[300]\ttraining's rmse: 440.889\tvalid_1's rmse: 328.031\n",
      "[400]\ttraining's rmse: 402.998\tvalid_1's rmse: 309.559\n",
      "[500]\ttraining's rmse: 378.09\tvalid_1's rmse: 302.956\n",
      "[600]\ttraining's rmse: 357.927\tvalid_1's rmse: 300.987\n",
      "[700]\ttraining's rmse: 340.101\tvalid_1's rmse: 299.111\n",
      "[800]\ttraining's rmse: 324.127\tvalid_1's rmse: 296.911\n",
      "[900]\ttraining's rmse: 309.361\tvalid_1's rmse: 294.144\n",
      "[1000]\ttraining's rmse: 296.167\tvalid_1's rmse: 291.153\n",
      "[1100]\ttraining's rmse: 283.724\tvalid_1's rmse: 289.694\n",
      "[1200]\ttraining's rmse: 273.216\tvalid_1's rmse: 289.351\n",
      "[1300]\ttraining's rmse: 262.672\tvalid_1's rmse: 288.302\n",
      "[1400]\ttraining's rmse: 253.381\tvalid_1's rmse: 286.781\n",
      "[1500]\ttraining's rmse: 244.448\tvalid_1's rmse: 284.958\n",
      "[1600]\ttraining's rmse: 236.877\tvalid_1's rmse: 285.143\n",
      "[1700]\ttraining's rmse: 229.121\tvalid_1's rmse: 284.877\n",
      "Early stopping, best iteration is:\n",
      "[1535]\ttraining's rmse: 242.062\tvalid_1's rmse: 284.609\n",
      "fold 1\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 598.647\tvalid_1's rmse: 725.824\n",
      "[200]\ttraining's rmse: 448.492\tvalid_1's rmse: 606.239\n",
      "[300]\ttraining's rmse: 396.154\tvalid_1's rmse: 569.904\n",
      "[400]\ttraining's rmse: 364.851\tvalid_1's rmse: 549.108\n",
      "[500]\ttraining's rmse: 341.696\tvalid_1's rmse: 538.102\n",
      "[600]\ttraining's rmse: 325.345\tvalid_1's rmse: 533.36\n",
      "[700]\ttraining's rmse: 309.462\tvalid_1's rmse: 532.982\n",
      "[800]\ttraining's rmse: 295.403\tvalid_1's rmse: 533.977\n",
      "[900]\ttraining's rmse: 284.76\tvalid_1's rmse: 532.412\n",
      "[1000]\ttraining's rmse: 272.735\tvalid_1's rmse: 531.275\n",
      "[1100]\ttraining's rmse: 262.029\tvalid_1's rmse: 530.307\n",
      "[1200]\ttraining's rmse: 253.786\tvalid_1's rmse: 529.548\n",
      "[1300]\ttraining's rmse: 244.905\tvalid_1's rmse: 529.152\n",
      "[1400]\ttraining's rmse: 236.163\tvalid_1's rmse: 528.676\n",
      "[1500]\ttraining's rmse: 229.598\tvalid_1's rmse: 528.312\n",
      "[1600]\ttraining's rmse: 221.705\tvalid_1's rmse: 528.476\n",
      "[1700]\ttraining's rmse: 214.221\tvalid_1's rmse: 527.81\n",
      "[1800]\ttraining's rmse: 207.758\tvalid_1's rmse: 527.761\n",
      "[1900]\ttraining's rmse: 201.85\tvalid_1's rmse: 527.853\n",
      "[2000]\ttraining's rmse: 195.111\tvalid_1's rmse: 527.693\n",
      "[2100]\ttraining's rmse: 188.81\tvalid_1's rmse: 527.439\n",
      "[2200]\ttraining's rmse: 184.393\tvalid_1's rmse: 527.699\n",
      "[2300]\ttraining's rmse: 178.548\tvalid_1's rmse: 527.794\n",
      "Early stopping, best iteration is:\n",
      "[2104]\ttraining's rmse: 188.751\tvalid_1's rmse: 527.402\n",
      "fold 2\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 652.983\tvalid_1's rmse: 416.205\n",
      "[200]\ttraining's rmse: 500.032\tvalid_1's rmse: 260.457\n",
      "[300]\ttraining's rmse: 445.9\tvalid_1's rmse: 223.93\n",
      "[400]\ttraining's rmse: 408.583\tvalid_1's rmse: 216.287\n",
      "[500]\ttraining's rmse: 382.656\tvalid_1's rmse: 216.164\n",
      "[600]\ttraining's rmse: 361.329\tvalid_1's rmse: 215.942\n",
      "Early stopping, best iteration is:\n",
      "[455]\ttraining's rmse: 393.422\tvalid_1's rmse: 215.279\n",
      "fold 3\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 631.905\tvalid_1's rmse: 610.649\n",
      "[200]\ttraining's rmse: 485.366\tvalid_1's rmse: 466.144\n",
      "[300]\ttraining's rmse: 435.108\tvalid_1's rmse: 422.673\n",
      "[400]\ttraining's rmse: 403.032\tvalid_1's rmse: 396.568\n",
      "[500]\ttraining's rmse: 378.774\tvalid_1's rmse: 386.336\n",
      "[600]\ttraining's rmse: 358.976\tvalid_1's rmse: 378.365\n",
      "[700]\ttraining's rmse: 342.267\tvalid_1's rmse: 372.062\n",
      "[800]\ttraining's rmse: 326.907\tvalid_1's rmse: 368.141\n",
      "[900]\ttraining's rmse: 312.091\tvalid_1's rmse: 363.802\n",
      "[1000]\ttraining's rmse: 299.246\tvalid_1's rmse: 362.427\n",
      "[1100]\ttraining's rmse: 288.021\tvalid_1's rmse: 361.16\n",
      "[1200]\ttraining's rmse: 277.757\tvalid_1's rmse: 359.928\n",
      "[1300]\ttraining's rmse: 268\tvalid_1's rmse: 358.734\n",
      "[1400]\ttraining's rmse: 258.502\tvalid_1's rmse: 358.592\n",
      "[1500]\ttraining's rmse: 249.385\tvalid_1's rmse: 358.614\n",
      "[1600]\ttraining's rmse: 240.586\tvalid_1's rmse: 358.242\n",
      "Early stopping, best iteration is:\n",
      "[1439]\ttraining's rmse: 254.984\tvalid_1's rmse: 357.91\n",
      "fold 4\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 556.582\tvalid_1's rmse: 889.773\n",
      "[200]\ttraining's rmse: 390.701\tvalid_1's rmse: 788.465\n",
      "[300]\ttraining's rmse: 333.209\tvalid_1's rmse: 760.552\n",
      "[400]\ttraining's rmse: 292.868\tvalid_1's rmse: 747.295\n",
      "[500]\ttraining's rmse: 263.277\tvalid_1's rmse: 741.846\n",
      "[600]\ttraining's rmse: 239.313\tvalid_1's rmse: 739.196\n",
      "[700]\ttraining's rmse: 220.323\tvalid_1's rmse: 736.512\n",
      "[800]\ttraining's rmse: 204.571\tvalid_1's rmse: 733.738\n",
      "[900]\ttraining's rmse: 192.47\tvalid_1's rmse: 731.703\n",
      "[1000]\ttraining's rmse: 179.817\tvalid_1's rmse: 730.739\n",
      "[1100]\ttraining's rmse: 168.218\tvalid_1's rmse: 730.371\n",
      "[1200]\ttraining's rmse: 158.624\tvalid_1's rmse: 729.601\n",
      "[1300]\ttraining's rmse: 149.754\tvalid_1's rmse: 728.812\n",
      "[1400]\ttraining's rmse: 142.318\tvalid_1's rmse: 728.189\n",
      "[1500]\ttraining's rmse: 137.166\tvalid_1's rmse: 727.416\n",
      "[1600]\ttraining's rmse: 130.913\tvalid_1's rmse: 727.723\n",
      "[1700]\ttraining's rmse: 125.265\tvalid_1's rmse: 727.432\n",
      "[1800]\ttraining's rmse: 120.03\tvalid_1's rmse: 727.221\n",
      "[1900]\ttraining's rmse: 114.966\tvalid_1's rmse: 726.605\n",
      "[2000]\ttraining's rmse: 110.413\tvalid_1's rmse: 726.524\n",
      "[2100]\ttraining's rmse: 107.191\tvalid_1's rmse: 726.34\n",
      "[2200]\ttraining's rmse: 103.474\tvalid_1's rmse: 725.983\n",
      "[2300]\ttraining's rmse: 99.9075\tvalid_1's rmse: 725.947\n",
      "[2400]\ttraining's rmse: 96.6856\tvalid_1's rmse: 725.6\n",
      "[2500]\ttraining's rmse: 93.5274\tvalid_1's rmse: 725.59\n",
      "[2600]\ttraining's rmse: 90.5718\tvalid_1's rmse: 725.316\n",
      "[2700]\ttraining's rmse: 88.6198\tvalid_1's rmse: 725.247\n",
      "[2800]\ttraining's rmse: 86.0087\tvalid_1's rmse: 725.143\n",
      "[2900]\ttraining's rmse: 83.4411\tvalid_1's rmse: 725.241\n",
      "Early stopping, best iteration is:\n",
      "[2792]\ttraining's rmse: 86.2332\tvalid_1's rmse: 725.051\n",
      "CV score: 460.28154\n"
     ]
    }
   ],
   "source": [
    "df = add_ml_lags(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Soft\\Anaconda3\\envs\\env_keras\\lib\\site-packages\\pandas\\core\\series.py:1828: RuntimeWarning: overflow encountered in multiply\n",
      "  result = com._values_from_object(self).round(decimals)\n"
     ]
    }
   ],
   "source": [
    "df = additional_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = df[df['target'].notnull()]\n",
    "test_df = df[df['target'].isnull()]\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds=11\n",
    "stratified = True\n",
    "if stratified:\n",
    "    folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=326)\n",
    "else:\n",
    "    folds = KFold(n_splits= num_folds, shuffle=True, random_state=326)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['target'] = 2**train_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.65977\ttest's rmse: 3.68858\n",
      "[200]\ttrain's rmse: 3.58028\ttest's rmse: 3.643\n",
      "[300]\ttrain's rmse: 3.53415\ttest's rmse: 3.62494\n",
      "[400]\ttrain's rmse: 3.49906\ttest's rmse: 3.61695\n",
      "[500]\ttrain's rmse: 3.47301\ttest's rmse: 3.61321\n",
      "[600]\ttrain's rmse: 3.45097\ttest's rmse: 3.61173\n",
      "[700]\ttrain's rmse: 3.43114\ttest's rmse: 3.61004\n",
      "[800]\ttrain's rmse: 3.41471\ttest's rmse: 3.60952\n",
      "[900]\ttrain's rmse: 3.39886\ttest's rmse: 3.6089\n",
      "[1000]\ttrain's rmse: 3.38305\ttest's rmse: 3.60866\n",
      "[1100]\ttrain's rmse: 3.36873\ttest's rmse: 3.60828\n",
      "[1200]\ttrain's rmse: 3.35424\ttest's rmse: 3.60803\n",
      "[1300]\ttrain's rmse: 3.33914\ttest's rmse: 3.60756\n",
      "[1400]\ttrain's rmse: 3.32486\ttest's rmse: 3.60687\n",
      "[1500]\ttrain's rmse: 3.3093\ttest's rmse: 3.60573\n",
      "[1600]\ttrain's rmse: 3.29518\ttest's rmse: 3.60466\n",
      "[1700]\ttrain's rmse: 3.2805\ttest's rmse: 3.60415\n",
      "[1800]\ttrain's rmse: 3.26702\ttest's rmse: 3.60437\n",
      "[1900]\ttrain's rmse: 3.25343\ttest's rmse: 3.60386\n",
      "[2000]\ttrain's rmse: 3.23921\ttest's rmse: 3.60417\n",
      "[2100]\ttrain's rmse: 3.22541\ttest's rmse: 3.60414\n",
      "Early stopping, best iteration is:\n",
      "[1915]\ttrain's rmse: 3.25124\ttest's rmse: 3.60374\n",
      "Fold  1 RMSE : 3.603744\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.65695\ttest's rmse: 3.69781\n",
      "[200]\ttrain's rmse: 3.57745\ttest's rmse: 3.66406\n",
      "[300]\ttrain's rmse: 3.53004\ttest's rmse: 3.65156\n",
      "[400]\ttrain's rmse: 3.4956\ttest's rmse: 3.64555\n",
      "[500]\ttrain's rmse: 3.46801\ttest's rmse: 3.64252\n",
      "[600]\ttrain's rmse: 3.44512\ttest's rmse: 3.64138\n",
      "[700]\ttrain's rmse: 3.42636\ttest's rmse: 3.64054\n",
      "[800]\ttrain's rmse: 3.40758\ttest's rmse: 3.64021\n",
      "[900]\ttrain's rmse: 3.39101\ttest's rmse: 3.63957\n",
      "[1000]\ttrain's rmse: 3.376\ttest's rmse: 3.63943\n",
      "[1100]\ttrain's rmse: 3.36007\ttest's rmse: 3.6399\n",
      "Early stopping, best iteration is:\n",
      "[995]\ttrain's rmse: 3.37677\ttest's rmse: 3.63934\n",
      "Fold  2 RMSE : 3.639345\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.65754\ttest's rmse: 3.70495\n",
      "[200]\ttrain's rmse: 3.57705\ttest's rmse: 3.66969\n",
      "[300]\ttrain's rmse: 3.52969\ttest's rmse: 3.65773\n",
      "[400]\ttrain's rmse: 3.49626\ttest's rmse: 3.65129\n",
      "[500]\ttrain's rmse: 3.46871\ttest's rmse: 3.64805\n",
      "[600]\ttrain's rmse: 3.44702\ttest's rmse: 3.64616\n",
      "[700]\ttrain's rmse: 3.42775\ttest's rmse: 3.64444\n",
      "[800]\ttrain's rmse: 3.40866\ttest's rmse: 3.6431\n",
      "[900]\ttrain's rmse: 3.39108\ttest's rmse: 3.64227\n",
      "[1000]\ttrain's rmse: 3.37431\ttest's rmse: 3.64235\n",
      "[1100]\ttrain's rmse: 3.35871\ttest's rmse: 3.64247\n",
      "Early stopping, best iteration is:\n",
      "[917]\ttrain's rmse: 3.38853\ttest's rmse: 3.64202\n",
      "Fold  3 RMSE : 3.642019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrain's rmse: 3.65412\ttest's rmse: 3.71643\n",
      "[200]\ttrain's rmse: 3.57167\ttest's rmse: 3.68887\n",
      "[300]\ttrain's rmse: 3.52279\ttest's rmse: 3.68141\n",
      "[400]\ttrain's rmse: 3.48804\ttest's rmse: 3.67914\n",
      "[500]\ttrain's rmse: 3.4597\ttest's rmse: 3.67855\n",
      "[600]\ttrain's rmse: 3.4361\ttest's rmse: 3.6777\n",
      "[700]\ttrain's rmse: 3.41585\ttest's rmse: 3.67769\n"
     ]
    }
   ],
   "source": [
    "# Create arrays and dataframes to store results\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    feats = [f for f in train_df.columns if f not in FEATS_EXCLUDED]\n",
    "\n",
    "    # k-fold\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['outliers'])):\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['target'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['target'].iloc[valid_idx]\n",
    "\n",
    "        # set data structure\n",
    "        lgb_train = lgb.Dataset(train_x,\n",
    "                                label=train_y,\n",
    "                                free_raw_data=False)\n",
    "        lgb_test = lgb.Dataset(valid_x,\n",
    "                               label=valid_y,\n",
    "                               free_raw_data=False)\n",
    "\n",
    "        # params optimized by optuna\n",
    "        params ={\n",
    "                'task': 'train',\n",
    "                'boosting': 'goss',\n",
    "                'objective': 'regression',\n",
    "                'metric': 'rmse',\n",
    "                'learning_rate': 0.01,\n",
    "                'subsample': 0.9855232997390695,\n",
    "                'max_depth': 7,\n",
    "                'top_rate': 0.9064148448434349,\n",
    "                'num_leaves': 63,\n",
    "                'min_child_weight': 41.9612869171337,\n",
    "                'other_rate': 0.0721768246018207,\n",
    "                'reg_alpha': 9.677537745007898,\n",
    "                'colsample_bytree': 0.5665320670155495,\n",
    "                'min_split_gain': 9.820197773625843,\n",
    "                'reg_lambda': 8.2532317400459,\n",
    "                'min_data_in_leaf': 21,\n",
    "                'verbose': -1,\n",
    "                'seed':int(2**n_fold),\n",
    "                'bagging_seed':int(2**n_fold),\n",
    "                'drop_seed':int(2**n_fold)\n",
    "                }\n",
    "\n",
    "        reg = lgb.train(\n",
    "                        params,\n",
    "                        lgb_train,\n",
    "                        valid_sets=[lgb_train, lgb_test],\n",
    "                        valid_names=['train', 'test'],\n",
    "                        num_boost_round=10000,\n",
    "                        early_stopping_rounds= 200,\n",
    "                        verbose_eval=100\n",
    "                        )\n",
    "\n",
    "        oof_preds[valid_idx] = reg.predict(valid_x, num_iteration=reg.best_iteration)\n",
    "        sub_preds += reg.predict(test_df[feats], num_iteration=reg.best_iteration) / folds.n_splits\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = np.log1p(reg.feature_importance(importance_type='gain', iteration=reg.best_iteration))\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        print('Fold %2d RMSE : %.6f' % (n_fold + 1, rmse(valid_y, oof_preds[valid_idx])))\n",
    "        del reg, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "    \n",
    "    print('LGBM RMSE: {:<8.5f}'.format(rmse(oof_preds, train_df['target'])))\n",
    "    #sub_preds = np.log2(sub_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CB 0--------------------------------------------------\n",
      "Fold  1 RMSE : 3.620180\n",
      "CB 1--------------------------------------------------\n",
      "Fold  2 RMSE : 3.643692\n",
      "CB 2--------------------------------------------------\n",
      "Fold  3 RMSE : 3.655465\n",
      "CB 3--------------------------------------------------\n",
      "Fold  4 RMSE : 3.670002\n",
      "CB 4--------------------------------------------------\n",
      "Fold  5 RMSE : 3.650339\n",
      "CB 5--------------------------------------------------\n",
      "Fold  6 RMSE : 3.653679\n",
      "CB 6--------------------------------------------------\n",
      "Fold  7 RMSE : 3.621149\n",
      "CB 7--------------------------------------------------\n",
      "Fold  8 RMSE : 3.701807\n",
      "CB 8--------------------------------------------------\n",
      "Fold  9 RMSE : 3.661767\n",
      "CB 9--------------------------------------------------\n",
      "Fold 10 RMSE : 3.624535\n",
      "CB 10--------------------------------------------------\n",
      "Fold 11 RMSE : 3.686924\n",
      "Cat Boost RMSE: 3.65368 \n"
     ]
    }
   ],
   "source": [
    "#Cat boost\n",
    "import catboost as cb\n",
    "\n",
    "oof_cb = np.zeros(train_df.shape[0])\n",
    "predictions_cb = np.zeros(test_df.shape[0])\n",
    "\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['outliers'])):\n",
    "    train_x, train_y = train_df[feats].iloc[train_idx], train_df['target'].iloc[train_idx]\n",
    "    valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['target'].iloc[valid_idx]\n",
    "    \n",
    "    # CatBoost Regressor estimator\n",
    "    model = cb.CatBoostRegressor(\n",
    "        learning_rate = 0.03,\n",
    "        iterations = 1000,\n",
    "        eval_metric = 'RMSE',\n",
    "        allow_writing_files = False,\n",
    "        od_type = 'Iter',\n",
    "        bagging_temperature = 0.2,\n",
    "        depth = 10,\n",
    "        od_wait = 20,\n",
    "        silent = True\n",
    "    )\n",
    "    \n",
    "    # Fit\n",
    "    model.fit(\n",
    "        train_x, train_y,\n",
    "        eval_set=[(train_x, train_y), (valid_x, valid_y)],\n",
    "        verbose=None,\n",
    "        early_stopping_rounds=100\n",
    "    )\n",
    "    \n",
    "    print(\"CB \" + str(n_fold) + \"-\" * 50)\n",
    "    \n",
    "    oof_cb[valid_idx] = model.predict(valid_x)\n",
    "    test_preds = model.predict(test_df[feats])\n",
    "    predictions_cb += test_preds / folds.n_splits\n",
    "    print('Fold %2d RMSE : %.6f' % (n_fold + 1, rmse(valid_y, oof_cb[valid_idx])))\n",
    "\n",
    "print('Cat Boost RMSE: {:<8.5f}'.format(rmse(oof_cb, train_df['target'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def find_best_weight(preds, target):\n",
    "    def _validate_func(weights):\n",
    "        ''' scipy minimize will pass the weights as a numpy array '''\n",
    "        final_prediction = 0\n",
    "        for weight, prediction in zip(weights, preds):\n",
    "                final_prediction += weight * prediction\n",
    "        return np.sqrt(mean_squared_error(final_prediction, target))\n",
    "\n",
    "    #the algorithms need a starting value, right not we chose 0.5 for all weights\n",
    "    #its better to choose many random starting points and run minimize a few times\n",
    "    starting_values = [0.5]*len(preds)\n",
    "\n",
    "    #adding constraints and a different solver as suggested by user 16universe\n",
    "    #https://kaggle2.blob.core.windows.net/forum-message-attachments/75655/2393/otto%20model%20weights.pdf?sv=2012-02-12&se=2015-05-03T21%3A22%3A17Z&sr=b&sp=r&sig=rkeA7EJC%2BiQ%2FJ%2BcMpcA4lYQLFh6ubNqs2XAkGtFsAv0%3D\n",
    "    cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n",
    "    #our weights are bound between 0 and 1\n",
    "    bounds = [(0, 1)] * len(preds)\n",
    "    \n",
    "    res = minimize(_validate_func, starting_values, method='Nelder-Mead', bounds=bounds, constraints=cons)\n",
    "    \n",
    "    print('Ensemble Score: {best_score}'.format(best_score=(1-res['fun'])))\n",
    "    print('Best Weights: {weights}'.format(weights=res['x']))\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Soft\\Anaconda3\\envs\\env_keras\\lib\\site-packages\\scipy\\optimize\\_minimize.py:518: RuntimeWarning: Method Nelder-Mead cannot handle constraints nor bounds.\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Score: -2.6483428403868827\n",
      "Best Weights: [0.62306617 0.42865847]\n"
     ]
    }
   ],
   "source": [
    "res = find_best_weight([oof_preds, oof_cb], train_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mix submission\n",
    "sub_df = pd.read_csv('D:\\Ellunium\\elo/sample_submission.csv')\n",
    "sub_df[\"target\"] = 0.62306617*sub_preds + 0.42865847*predictions_cb\n",
    "sub_df.to_csv(\"D:\\Ellunium\\elo/submission_elo_strat_lgb_cat.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple submission\n",
    "sub_df = pd.read_csv('D:\\Ellunium\\elo/sample_submission.csv')\n",
    "sub_df[\"target\"] = sub_preds\n",
    "sub_df.to_csv(\"D:\\Ellunium\\elo/submission_elo_strat_lgb_3_64489.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df = train_df.reset_index()\n",
    "#test_df = test_df.reset_index()\n",
    "train_df.reset_index().to_csv(\"D:\\Ellunium\\elo/train_data_clean_17_02.csv\", index=False)\n",
    "test_df.reset_index().to_csv(\"D:\\Ellunium\\elo/test_data_clean_17_02.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>card_id</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>first_active_month</th>\n",
       "      <th>outliers</th>\n",
       "      <th>target</th>\n",
       "      <th>quarter</th>\n",
       "      <th>elapsed_time</th>\n",
       "      <th>days_feature1</th>\n",
       "      <th>...</th>\n",
       "      <th>price_max</th>\n",
       "      <th>duration_mean</th>\n",
       "      <th>duration_min</th>\n",
       "      <th>duration_max</th>\n",
       "      <th>amount_month_ratio_mean</th>\n",
       "      <th>amount_month_ratio_min</th>\n",
       "      <th>amount_month_ratio_max</th>\n",
       "      <th>new_CLV</th>\n",
       "      <th>hist_CLV</th>\n",
       "      <th>CLV_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C_ID_92a2005557</td>\n",
       "      <td>0.013145</td>\n",
       "      <td>0.008752</td>\n",
       "      <td>0.011428</td>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.820283</td>\n",
       "      <td>2.0</td>\n",
       "      <td>626.0</td>\n",
       "      <td>3130.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2300.000000</td>\n",
       "      <td>2184.511719</td>\n",
       "      <td>225.000000</td>\n",
       "      <td>27600.000000</td>\n",
       "      <td>15.875774</td>\n",
       "      <td>1.780273</td>\n",
       "      <td>193.932297</td>\n",
       "      <td>5149.522065</td>\n",
       "      <td>414840.406250</td>\n",
       "      <td>0.012413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C_ID_3d0044924f</td>\n",
       "      <td>0.010712</td>\n",
       "      <td>0.011385</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.392913</td>\n",
       "      <td>1.0</td>\n",
       "      <td>777.0</td>\n",
       "      <td>3108.0</td>\n",
       "      <td>...</td>\n",
       "      <td>328.041809</td>\n",
       "      <td>1432.088135</td>\n",
       "      <td>103.870003</td>\n",
       "      <td>46880.000000</td>\n",
       "      <td>8.720010</td>\n",
       "      <td>0.614258</td>\n",
       "      <td>277.765747</td>\n",
       "      <td>39.105829</td>\n",
       "      <td>936333.375000</td>\n",
       "      <td>0.000042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C_ID_d639edf6cd</td>\n",
       "      <td>0.010610</td>\n",
       "      <td>0.008752</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>2016-08-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.688056</td>\n",
       "      <td>3.0</td>\n",
       "      <td>930.0</td>\n",
       "      <td>1860.0</td>\n",
       "      <td>...</td>\n",
       "      <td>inf</td>\n",
       "      <td>870.480957</td>\n",
       "      <td>474.919983</td>\n",
       "      <td>4741.000000</td>\n",
       "      <td>6.761347</td>\n",
       "      <td>3.748047</td>\n",
       "      <td>39.181995</td>\n",
       "      <td>2.818182</td>\n",
       "      <td>7159.795898</td>\n",
       "      <td>0.000394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C_ID_186d6a6901</td>\n",
       "      <td>0.010712</td>\n",
       "      <td>0.014166</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142495</td>\n",
       "      <td>3.0</td>\n",
       "      <td>534.0</td>\n",
       "      <td>2136.0</td>\n",
       "      <td>...</td>\n",
       "      <td>394.747498</td>\n",
       "      <td>1441.479248</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>17488.791016</td>\n",
       "      <td>10.667797</td>\n",
       "      <td>1.041992</td>\n",
       "      <td>142.644547</td>\n",
       "      <td>225.463827</td>\n",
       "      <td>35165.296875</td>\n",
       "      <td>0.006412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C_ID_cdbd2c0db2</td>\n",
       "      <td>0.008058</td>\n",
       "      <td>0.014166</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.159749</td>\n",
       "      <td>4.0</td>\n",
       "      <td>473.0</td>\n",
       "      <td>473.0</td>\n",
       "      <td>...</td>\n",
       "      <td>434.362854</td>\n",
       "      <td>4386.531738</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>67689.000000</td>\n",
       "      <td>33.357674</td>\n",
       "      <td>0.458496</td>\n",
       "      <td>546.797485</td>\n",
       "      <td>14130.305261</td>\n",
       "      <td>383071.906250</td>\n",
       "      <td>0.036887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 243 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           card_id  feature_1  feature_2  feature_3 first_active_month  \\\n",
       "0  C_ID_92a2005557   0.013145   0.008752   0.011428         2017-06-01   \n",
       "1  C_ID_3d0044924f   0.010712   0.011385   0.010283         2017-01-01   \n",
       "2  C_ID_d639edf6cd   0.010610   0.008752   0.010283         2016-08-01   \n",
       "3  C_ID_186d6a6901   0.010712   0.014166   0.010283         2017-09-01   \n",
       "4  C_ID_cdbd2c0db2   0.008058   0.014166   0.010283         2017-11-01   \n",
       "\n",
       "   outliers    target  quarter  elapsed_time  days_feature1    ...      \\\n",
       "0       0.0 -0.820283      2.0         626.0         3130.0    ...       \n",
       "1       0.0  0.392913      1.0         777.0         3108.0    ...       \n",
       "2       0.0  0.688056      3.0         930.0         1860.0    ...       \n",
       "3       0.0  0.142495      3.0         534.0         2136.0    ...       \n",
       "4       0.0 -0.159749      4.0         473.0          473.0    ...       \n",
       "\n",
       "     price_max  duration_mean  duration_min  duration_max  \\\n",
       "0  2300.000000    2184.511719    225.000000  27600.000000   \n",
       "1   328.041809    1432.088135    103.870003  46880.000000   \n",
       "2          inf     870.480957    474.919983   4741.000000   \n",
       "3   394.747498    1441.479248    150.000000  17488.791016   \n",
       "4   434.362854    4386.531738     66.000000  67689.000000   \n",
       "\n",
       "   amount_month_ratio_mean  amount_month_ratio_min  amount_month_ratio_max  \\\n",
       "0                15.875774                1.780273              193.932297   \n",
       "1                 8.720010                0.614258              277.765747   \n",
       "2                 6.761347                3.748047               39.181995   \n",
       "3                10.667797                1.041992              142.644547   \n",
       "4                33.357674                0.458496              546.797485   \n",
       "\n",
       "        new_CLV       hist_CLV  CLV_ratio  \n",
       "0   5149.522065  414840.406250   0.012413  \n",
       "1     39.105829  936333.375000   0.000042  \n",
       "2      2.818182    7159.795898   0.000394  \n",
       "3    225.463827   35165.296875   0.006412  \n",
       "4  14130.305261  383071.906250   0.036887  \n",
       "\n",
       "[5 rows x 243 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
